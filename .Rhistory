#---------------------#
curr_status[which(curr_status[,1] == h),2:26] <-c(SR[1,],SR[2,],hist.escape,habitat,ltm,COSEWIC[1,], COSEWIC[2,], exploit[1,],exploit[2,])
stat.met.bch[,1] <-  as.character(unique(cu_master.all.spp$cuid))
stat.met.bch [which(stat.met.bch[,1] == h),2] <- ifelse(is.na(AvgEsc),"NA", round(AvgEsc,digits=0))
stat.met.bch [which(stat.met.bch[,1] == h),7] <- ifelse(is.na(AvgER),"NA", round(AvgER,digits=2))
if(is.na(bench[1,1])==TRUE){stat.met.bch [which(stat.met.bch[,1] == h),8:13] <- "NA" }else{ stat.met.bch [which(stat.met.bch[,1] == h),8:13] <- as.numeric(c(bench[2,1:3],bench[3,1:3])) }
curr_status[which(curr_status[,1] == h),27] <- as.character(cu_master$cuid[cu_master$cuid==h])
curr_status[which(curr_status[,1] == h),28] <- as.character(cu_master$species[cu_master$cuid==h])
stat.met.bch [which(stat.met.bch[,1] == h),21] <- as.character(cu_master$cuid[cu_master$cuid==h])
stat.met.bch [which(stat.met.bch[,1] == h),22] <- as.character(cu_master$species[cu_master$cuid==h])
}
}
# output dataset 101
ds.101 <- cbind(curr_status[,27],curr_status[,28],curr_status[,-1])
ds.101 <- ds.101[,-28]
ds.101 <- ds.101[,-28]
colnames(ds.101)[1:2] <- c("CUID", "Species")
write.csv(ds.101,file=paste("Output/dataset_101.",date,".csv",sep=""))
# output dataset 102
ds.102 <- cbind(stat.met.bch[,21], stat.met.bch[,22], stat.met.bch[,-1])
ds.102 <- ds.102[,-22]
ds.102 <- ds.102[,-22]
colnames(ds.102)[1:2] <- c("CUID", "Species")
write.csv(ds.102,file=paste("Output/dataset_102.",date,".csv",sep=""))
# output dataset 103
ds.103.a <- as.data.frame(run.avg.escape)
ds.103.a [ds.103.a == "-99999"] <- NA
ds.103 <- melt(ds.103.a, id.vars = c("cu","location","species"),variable.name = "year", value.name = "avgescapelog")
ds.103  <- ds.103[,-1]
colnames(ds.103) <- c("CUID","Species","Year","avgescapelog")
write.csv(ds.103,file=paste("Output/dataset_103.",date,".csv",sep=""))
# output dataset 202
ds.202.a <- as.data.frame(trends)
ds.202 <- cbind(trends[,5], trends[,6], trends[,-1])
ds.202 <- ds.202[,-6]
ds.202 <- ds.202[,-6]
colnames(ds.202)[1:2] <- c("CUID", "Species")
write.csv(ds.202,file=paste("Output/dataset_202.",date,".csv",sep=""))
# output dataset 279
ds.279 <- as.data.frame(curr_abun)
ds.279 <- cbind(ds.279[,6], ds.279[,7], ds.279[,-1])
ds.279 <- ds.279[,-7]
ds.279 <- ds.279[,-7]
colnames(ds.279) <- c("CUID", "Species","Abundance","Start year","End year","Rule")
write.csv(ds.279,file=paste("Output/dataset_279.",date,".csv",sep=""))
library(dplyr)
require(reshape2)
require(corrplot)
library(plotrix)
require(reshape2)
library(dplyr)
library(broom)
setwd("~/Dropbox (Salmon Watersheds)/X Drive/1_PROJECTS/1_Active/Haida Gwaii/Data and Assessments/Haida Gwaii 2017 status assessment/Output/")
abund_file <- read.csv("dataset_103.Oct062021.csv", header=T)
dat_510 <- subset (abund_file, CUID=='801')
dat_510 <- na.omit(dat_510)
Yr <- c(1:57)
mod_510 <- lm(dat_510$avgescapelog~Yr)
summary(mod_510)
predict(mod_510)
df <- predict(mod)
df <- predict(mod_510)
x <- df[1]
y <- tail(df,1)
(exp(y) - exp(x))/exp(x)*100
dat_510 <- subset (abund_file, CUID=='802')
dat_510 <- na.omit(dat_510)
Yr <- c(1:63)
mod_510 <- lm(dat_510$avgescapelog~Yr)
summary(mod_510)
df <- predict(mod_510)
x <- df[1]
y <- tail(df,1)
(exp(y) - exp(x))/exp(x)*100
dat_510 <- subset (abund_file, CUID=='803')
dat_510 <- na.omit(dat_510)
Yr <- c(1:63)
mod_510 <- lm(dat_510$avgescapelog~Yr)
summary(mod_510)
df <- predict(mod_510)
x <- df[1]
y <- tail(df,1)
(exp(y) - exp(x))/exp(x)*100
dat_510 <- subset (abund_file, CUID=='804')
dat_510 <- na.omit(dat_510)
Yr <- c(1:63)
mod_510 <- lm(dat_510$avgescapelog~Yr)
summary(mod_510)
df <- predict(mod_510)
x <- df[1]
y <- tail(df,1)
(exp(y) - exp(x))/exp(x)*100
dat_510 <- subset (abund_file, CUID=='807')
dat_510 <- na.omit(dat_510)
Yr <- c(1:57)
mod_510 <- lm(dat_510$avgescapelog~Yr)
summary(mod_510)
df <- predict(mod_510)
x <- df[1]
y <- tail(df,1)
(exp(y) - exp(x))/exp(x)*100
dat_510 <- subset (abund_file, CUID=='808')
dat_510 <- na.omit(dat_510)
Yr <- c(1:60)
mod_510 <- lm(dat_510$avgescapelog~Yr)
summary(mod_510)
df <- predict(mod_510)
x <- df[1]
y <- tail(df,1)
(exp(y) - exp(x))/exp(x)*100
dat_510 <- subset (abund_file, CUID=='809')
dat_510 <- na.omit(dat_510)
Yr <- c(1:60)
mod_510 <- lm(dat_510$avgescapelog~Yr)
summary(mod_510)
df <- predict(mod_510)
x <- df[1]
y <- tail(df,1)
(exp(y) - exp(x))/exp(x)*100
dat_510 <- subset (abund_file, CUID=='811')
dat_510 <- na.omit(dat_510)
Yr <- c(1:60)
mod_510 <- lm(dat_510$avgescapelog~Yr)
dat_510 <- subset (abund_file, CUID=='811')
dat_510 <- na.omit(dat_510)
Yr <- c(1:57)
mod_510 <- lm(dat_510$avgescapelog~Yr)
summary(mod_510)
df <- predict(mod_510)
x <- df[1]
y <- tail(df,1)
(exp(y) - exp(x))/exp(x)*100
dat_510 <- subset (abund_file, CUID=='820')
dat_510 <- na.omit(dat_510)
Yr <- c(1:57)
mod_510 <- lm(dat_510$avgescapelog~Yr)
summary(mod_510)
df <- predict(mod_510)
x <- df[1]
y <- tail(df,1)
(exp(y) - exp(x))/exp(x)*100
dat_510 <- subset (abund_file, CUID=='823')
dat_510 <- na.omit(dat_510)
Yr <- c(1:57)
mod_510 <- lm(dat_510$avgescapelog~Yr)
dat_510 <- subset (abund_file, CUID=='823')
dat_510 <- na.omit(dat_510)
Yr <- c(1:63)
mod_510 <- lm(dat_510$avgescapelog~Yr)
summary(mod_510)
df <- predict(mod_510)
x <- df[1]
y <- tail(df,1)
(exp(y) - exp(x))/exp(x)*100
dat_510 <- subset (abund_file, CUID=='825')
dat_510 <- na.omit(dat_510)
Yr <- c(1:61)
mod_510 <- lm(dat_510$avgescapelog~Yr)
summary(mod_510)
df <- predict(mod_510)
x <- df[1]
y <- tail(df,1)
(exp(y) - exp(x))/exp(x)*100
dat_510 <- subset (abund_file, CUID=='826')
dat_510 <- na.omit(dat_510)
Yr <- c(1:63)
mod_510 <- lm(dat_510$avgescapelog~Yr)
summary(mod_510)
df <- predict(mod_510)
x <- df[1]
y <- tail(df,1)
(exp(y) - exp(x))/exp(x)*100
dat_510 <- subset (abund_file, CUID=='827')
dat_510 <- na.omit(dat_510)
Yr <- c(1:63)
mod_510 <- lm(dat_510$avgescapelog~Yr)
summary(mod_510)
df <- predict(mod_510)
x <- df[1]
y <- tail(df,1)
(exp(y) - exp(x))/exp(x)*100
### Loading packages ###
library(plyr)
library(dplyr)
library(tibble)
library(scales)
library(ggplot2)
library(readxl)
library(reshape2)
library(stringr)
library(tidyr)
# --- Input (change directory paths accordingly) --- #
rm(list=ls())
graphics.off()
# Directory to draw data from #
dir.data <- "~/Dropbox (Salmon Watersheds)/X Drive/1_PROJECTS/Population Methods and Analysis/New features/PSE New Population Features/Data"
# Directory to write data to #
dir.out <- "~/Dropbox (Salmon Watersheds)/X Drive/1_PROJECTS/Population Methods and Analysis/New features/PSE New Population Features/Results"
# Directory to draw reference outputs from #
dir.ref <- "~/Dropbox (Salmon Watersheds)/X Drive/1_PROJECTS/Population Methods and Analysis/New features/PSE New Population Features/Reference"
# Template data frame #
setwd(dir.ref)
refdat <- read.delim("NCC_Streams_13March2016_KKE.txt", header=TRUE, na.string="")
# Set up column names for empty destination dataframe #
names <- c(names(refdat)[1:45], c(1926:2019))
# NuSEDS data and CU site data #
setwd(dir.data)
nuseds <- read.csv("NuSEDS_20210513.csv", header=TRUE, stringsAsFactors=FALSE)
# NuSEDS list of CUs #
cu.sites <- read.csv("conservation_unit_system_sites.csv", header=TRUE, stringsAsFactors=FALSE)
names(cu.sites)[1] <- "ID"
# PSF list of CUs #
psf.cu <- read.csv("All_Regions_CU_decoder.csv", header=TRUE, stringsAsFactors = FALSE)
# PSF list of streams with streamids #
psf.streams <- read.csv("All_regions_streams_decoder.csv", header=TRUE, stringsAsFactors = FALSE)
# - Step 2: Calculate stream escapement - #
# - Code adapted from LGL script: "CalculateStreamEscapement.fn.R" - #
a<- subset(nuseds, POP_ID==40024)
z<-select(nuseds, SPECIES, POP_ID, ANALYSIS_YR, ESTIMATE_CLASSIFICATION, ESTIMATE_METHOD, WATERBODY, MAX_ESTIMATE)
a<- subset(z, POP_ID==40023)
unique(z$ESTIMATE_CLASSIFICATION)
z <- z[z$ESTIMATE_CLASSIFICATION!= "NO SURVEY THIS YEAR",]
unique(z$ESTIMATE_CLASSIFICATION)
z <- z[z$ESTIMATE_CLASSIFICATION!= "",]
z <- z[z$ESTIMATE_CLASSIFICATION!= "NO SURVEY",]
unique(z$ESTIMATE_CLASSIFICATION)
z$ESTIMATE_CLASSIFICATION[z$ESTIMATE_CLASSIFICATION=="UNKNOWN"] <- 0
z$ESTIMATE_CLASSIFICATION[z$ESTIMATE_CLASSIFICATION=="PRESENCE-ABSENCE (TYPE-6)"] <- 6
z$ESTIMATE_CLASSIFICATION[z$ESTIMATE_CLASSIFICATION=="RELATIVE ABUNDANCE (TYPE-5)"] <- 5
z$ESTIMATE_CLASSIFICATION[z$ESTIMATE_CLASSIFICATION=="RELATIVE ABUNDANCE (TYPE-4)"] <- 4
z$ESTIMATE_CLASSIFICATION[z$ESTIMATE_CLASSIFICATION=="RELATIVE ABUNDANCE (TYPE-3)"] <- 3
z$ESTIMATE_CLASSIFICATION[z$ESTIMATE_CLASSIFICATION=="TRUE ABUNDANCE (TYPE-2)"] <- 2
z$ESTIMATE_CLASSIFICATION[z$ESTIMATE_CLASSIFICATION=="TRUE ABUNDANCE (TYPE-1)"] <- 1
z$ESTIMATE_CLASSIFICATION[z$ESTIMATE_CLASSIFICATION=="RELATIVE: VARYING MULTI-YEAR METHODS"] <- 0
z$ESTIMATE_CLASSIFICATION[z$ESTIMATE_CLASSIFICATION=="RELATIVE: CONSTANT MULTI-YEAR METHODS"] <- 0
z$ESTIMATE_CLASSIFICATION <- as.numeric(z$ESTIMATE_CLASSIFICATION)
unique(z$ESTIMATE_METHOD)
df1 <- select(cu.sites, POP_ID, IS_INDICATOR, FULL_CU_IN, CU_NAME)
zz <- merge(x=z, y=df1, by = "POP_ID")
zz <- zz[zz$SPECIES!= "Atlantic",]
zz <- zz[zz$SPECIES!= "Steelhead",]
zz <- zz[zz$SPECIES!= "Kokanee",]
zz$DQ_SCORE = zz$ESTIMATE_CLASSIFICATION
zz$DQ_SCORE[zz$DQ_SCORE=="0"] <- "Unknown"
zz$DQ_SCORE[zz$DQ_SCORE=="1"] <- "High"
zz$DQ_SCORE[zz$DQ_SCORE=="2"] <- "Medium-High"
zz$DQ_SCORE[zz$DQ_SCORE=="3"] <- "Medium"
zz$DQ_SCORE[zz$DQ_SCORE=="4"] <- "Medium-Low"
zz$DQ_SCORE[zz$DQ_SCORE=="5"] <- "Low"
zz$DQ_SCORE[zz$DQ_SCORE=="6"] <- "Low"
df3 <- select(psf.cu, cuid, FULL_CU_IN)
zz <- merge(x = zz, y = df3, by = "FULL_CU_IN", all.x = TRUE)
df4<- select(psf.streams, to, streamname, cuid)
zzz<- left_join(zz, df4, by = c('WATERBODY' = 'streamname', "cuid" = "cuid"))
zzz<- zzz[complete.cases(zzz), ]
subset( zz, FULL_CU_IN=="CM-25")
zzz<-select(zzz, POP_ID,ANALYSIS_YR,ESTIMATE_CLASSIFICATION,ESTIMATE_METHOD,IS_INDICATOR,CU_NAME,DQ_SCORE,cuid,to)
?colnames
colnames(zzz) <- c("POP_ID", "year","survey_score","survey_method", "indicator","cu_name","survey_qual","cuid","streamid")
zzz <- select(zzz,  year,survey_score,survey_method,indicator,cu_name,survey_qual,cuid,streamid)
setwd(dir.out)
write.csv(zzz, "Stream_Level_DQ_20211006.csv", row.names=FALSE)
rm(list=ls())
graphics.off()
# Directory to draw data from #
dir.data <- "~/Dropbox (Salmon Watersheds)/X Drive/1_PROJECTS/Population Methods and Analysis/New features/PSE New Population Features/Data"
# Directory to write data to #
dir.out <- "~/Dropbox (Salmon Watersheds)/X Drive/1_PROJECTS/Population Methods and Analysis/New features/PSE New Population Features/Results"
# Directory to draw reference outputs from #
dir.ref <- "~/Dropbox (Salmon Watersheds)/X Drive/1_PROJECTS/Population Methods and Analysis/New features/PSE New Population Features/Reference"
# --- Loading data --- #
# Template data frame #
setwd(dir.ref)
refdat <- read.delim("NCC_Streams_13March2016_KKE.txt", header=TRUE, na.string="")
# Set up column names for empty destination dataframe #
names <- c(names(refdat)[1:45], c(1926:2019))
# NuSEDS data and CU site data #
setwd(dir.data)
nuseds <- read.csv("NuSEDS_20210513.csv", header=TRUE, stringsAsFactors=FALSE)
# NuSEDS list of CUs #
cu.sites <- read.csv("conservation_unit_system_sites.csv", header=TRUE, stringsAsFactors=FALSE)
names(cu.sites)[1] <- "ID"
# PSF list of CUs #
psf.cu <- read.csv("All_Regions_CU_decoder.csv", header=TRUE, stringsAsFactors = FALSE)
# PSF list of streams with streamids #
psf.streams <- read.csv("All_regions_streams_decoder.csv", header=TRUE, stringsAsFactors = FALSE)
# - Step 2: Calculate stream escapement - #
# - Code adapted from LGL script: "CalculateStreamEscapement.fn.R" - #
a<- subset(nuseds, POP_ID==40024)
z<-select(nuseds, SPECIES, POP_ID, ANALYSIS_YR, ESTIMATE_CLASSIFICATION, ESTIMATE_METHOD, WATERBODY, MAX_ESTIMATE)
a<- subset(z, POP_ID==40023)
unique(z$ESTIMATE_CLASSIFICATION)
z <- z[z$ESTIMATE_CLASSIFICATION!= "NO SURVEY THIS YEAR",]
unique(z$ESTIMATE_CLASSIFICATION)
z <- z[z$ESTIMATE_CLASSIFICATION!= "",]
z <- z[z$ESTIMATE_CLASSIFICATION!= "NO SURVEY",]
unique(z$ESTIMATE_CLASSIFICATION)
z$ESTIMATE_CLASSIFICATION[z$ESTIMATE_CLASSIFICATION=="UNKNOWN"] <- 0
z$ESTIMATE_CLASSIFICATION[z$ESTIMATE_CLASSIFICATION=="PRESENCE-ABSENCE (TYPE-6)"] <- 6
z$ESTIMATE_CLASSIFICATION[z$ESTIMATE_CLASSIFICATION=="RELATIVE ABUNDANCE (TYPE-5)"] <- 5
z$ESTIMATE_CLASSIFICATION[z$ESTIMATE_CLASSIFICATION=="RELATIVE ABUNDANCE (TYPE-4)"] <- 4
z$ESTIMATE_CLASSIFICATION[z$ESTIMATE_CLASSIFICATION=="RELATIVE ABUNDANCE (TYPE-3)"] <- 3
z$ESTIMATE_CLASSIFICATION[z$ESTIMATE_CLASSIFICATION=="TRUE ABUNDANCE (TYPE-2)"] <- 2
z$ESTIMATE_CLASSIFICATION[z$ESTIMATE_CLASSIFICATION=="TRUE ABUNDANCE (TYPE-1)"] <- 1
z$ESTIMATE_CLASSIFICATION[z$ESTIMATE_CLASSIFICATION=="RELATIVE: VARYING MULTI-YEAR METHODS"] <- 0
z$ESTIMATE_CLASSIFICATION[z$ESTIMATE_CLASSIFICATION=="RELATIVE: CONSTANT MULTI-YEAR METHODS"] <- 0
z$ESTIMATE_CLASSIFICATION <- as.numeric(z$ESTIMATE_CLASSIFICATION)
unique(z$ESTIMATE_METHOD)
df1 <- select(cu.sites, POP_ID, IS_INDICATOR, FULL_CU_IN, CU_NAME)
zz <- merge(x=z, y=df1, by = "POP_ID")
zz <- zz[zz$SPECIES!= "Atlantic",]
zz <- zz[zz$SPECIES!= "Steelhead",]
zz <- zz[zz$SPECIES!= "Kokanee",]
zz$DQ_SCORE = zz$ESTIMATE_CLASSIFICATION
zz$DQ_SCORE[zz$DQ_SCORE=="0"] <- "Unknown"
zz$DQ_SCORE[zz$DQ_SCORE=="1"] <- "High"
zz$DQ_SCORE[zz$DQ_SCORE=="2"] <- "Medium-High"
zz$DQ_SCORE[zz$DQ_SCORE=="3"] <- "Medium"
zz$DQ_SCORE[zz$DQ_SCORE=="4"] <- "Medium-Low"
zz$DQ_SCORE[zz$DQ_SCORE=="5"] <- "Low"
zz$DQ_SCORE[zz$DQ_SCORE=="6"] <- "Low"
df3 <- select(psf.cu, cuid, FULL_CU_IN)
zz <- merge(x = zz, y = df3, by = "FULL_CU_IN", all.x = TRUE)
df4<- select(psf.streams, to, streamname, cuid)
zzz<- left_join(zz, df4, by = c('WATERBODY' = 'streamname', "cuid" = "cuid"))
zzz<- zzz[complete.cases(zzz), ]
subset( zz, FULL_CU_IN=="CM-25")
zzz<-select(zzz, POP_ID,ANALYSIS_YR,ESTIMATE_CLASSIFICATION,ESTIMATE_METHOD,IS_INDICATOR,CU_NAME,DQ_SCORE,cuid,to)
?colnames
colnames(zzz) <- c("POP_ID", "year","survey_score","survey_method", "indicator","cu_name","survey_qual","cuid","streamid")
zzz <- select(zzz,  year,survey_score,survey_method,indicator,cu_name,survey_qual,cuid,streamid)
setwd(dir.out)
write.csv(zzz, "Stream_Level_DQ_20211006.csv", row.names=FALSE)
rm(list = ls(all=TRUE)); #Remove all the objects in the memory
setwd("~/Dropbox (Salmon Watersheds)/X Drive/1_PROJECTS/Population Methods and Analysis/ProgRegSummaries")
library(tidyverse)
library(broom)
library(geosphere)
#read in CU level data
cc_file <- read.csv("data/dataset_1part1.Dec072020_CC.csv", header = T)
fraser_file <- read.csv("data/dataset_1part1.Jul282020_Fraser.csv", header = T)
vimi_file <- read.csv("data/dataset_1part1.Jul282021_VIMI.csv", header = T)
nass_file <- read.csv("data/dataset_1part1.Dec092020_Nass.csv", header = T)
skeena_file <- read.csv("data/dataset_1part1.Dec092020_Skeena.csv", header = T)
hg_file <- read.csv("data/dataset_1part1.Sep272021_HG.csv", header = T)
columbia_file <- read.csv("data/dataset_1part1.NOV272019_Columbia.csv", header = T)
# combine files from each region
cu_dat <- rbind(cc_file,fraser_file,vimi_file,nass_file,skeena_file,hg_file,columbia_file)
hg_file <- read.csv("data/dataset_1part1.Sep272021_HG.csv", header = T)
# combine files from each region
cu_dat <- rbind(cc_file,fraser_file,vimi_file,nass_file,skeena_file,hg_file,columbia_file)
cu_dat <- select(cu_dat,CUID,Species,Year,Total.run,Region)
#check species names
unique(cu_dat$Species)
cu_dat$Species[cu_dat$Species=="River Sockeye"] <- "Sockeye"
cu_dat$Species[cu_dat$Species=="Lake Sockeye"] <- "Sockeye"
#### 2 Generate files needed for Provincial summaries
#Select only the years that have CUs that comprise 80% of the maximum historical run size
# ------------------------------------------------------------------------------
cu_dat <- as_tibble(cu_dat)
max_runs <-
cu_dat %>%
drop_na(Total.run) %>%
group_by(CUID, Species) %>%
slice(which.max(Total.run)) %>%
dplyr::rename(max_run = Total.run) %>%
ungroup()
total_max_run <-
max_runs %>%
group_by(Species, Region) %>%
summarise(total_max_run = sum(max_run)) %>%
ungroup()
test <-
cu_dat %>%
drop_na(Total.run) %>%
left_join(., max_runs %>% select(-Year)) %>%
left_join(., total_max_run) %>%
group_by(Year, Species, Region) %>%
arrange(Year, Species, Region, -max_run) %>%
mutate(c1 = cumsum(max_run)) %>%
mutate(percent = c1 / total_max_run) %>%
mutate(pop1 = lag(percent), pop_cur = percent) %>%
# Identify if there is a CU that hits the 80% cutoff
mutate(cutoff = ifelse(pop_cur >= 0.8 & pop1 < 0.8, TRUE, FALSE)) %>%
ungroup() %>%
# Reorder the columns to make checking easier
select(CUID, Region, Species, Year, Total.run, c1, total_max_run, percent, pop1, pop_cur, cutoff)
View(hg_file)
View(vimi_file)
max_runs <-
cu_dat %>%
drop_na(Total.run) %>%
group_by(CUID, Species) %>%
slice(which.max(Total.run)) %>%
dplyr::rename(max_run = Total.run) %>%
ungroup()
rm(list = ls(all=TRUE)); #Remove all the objects in the memory
setwd("~/Dropbox (Salmon Watersheds)/X Drive/1_PROJECTS/Population Methods and Analysis/ProgRegSummaries")
library(tidyverse)
library(broom)
library(geosphere)
#### 1 Read in and manipulateddata
#read in CU level data
cc_file <- read.csv("data/dataset_1part1.Dec072020_CC.csv", header = T)
fraser_file <- read.csv("data/dataset_1part1.Jul282020_Fraser.csv", header = T)
vimi_file <- read.csv("data/dataset_1part1.Jul282021_VIMI.csv", header = T)
nass_file <- read.csv("data/dataset_1part1.Dec092020_Nass.csv", header = T)
skeena_file <- read.csv("data/dataset_1part1.Dec092020_Skeena.csv", header = T)
hg_file <- read.csv("data/dataset_1part1.Sep272021_HG.csv", header = T)
columbia_file <- read.csv("data/dataset_1part1.NOV272019_Columbia.csv", header = T)
# combine files from each region
cu_dat <- rbind(cc_file,fraser_file,vimi_file,nass_file,skeena_file,hg_file,columbia_file)
cu_dat <- select(cu_dat,CUID,Species,Year,Total.run,Region)
#check species names
unique(cu_dat$Species)
cu_dat$Species[cu_dat$Species=="River Sockeye"] <- "Sockeye"
cu_dat$Species[cu_dat$Species=="Lake Sockeye"] <- "Sockeye"
#### 2 Generate files needed for Provincial summaries
#Select only the years that have CUs that comprise 80% of the maximum historical run size
# ------------------------------------------------------------------------------
cu_dat <- as_tibble(cu_dat)
max_runs <-
cu_dat %>%
drop_na(Total.run) %>%
group_by(CUID, Species) %>%
slice(which.max(Total.run)) %>%
dplyr::rename(max_run = Total.run) %>%
ungroup()
total_max_run <-
max_runs %>%
group_by(Species, Region) %>%
summarise(total_max_run = sum(max_run)) %>%
ungroup()
test <-
cu_dat %>%
drop_na(Total.run) %>%
left_join(., max_runs %>% select(-Year)) %>%
left_join(., total_max_run) %>%
group_by(Year, Species, Region) %>%
arrange(Year, Species, Region, -max_run) %>%
mutate(c1 = cumsum(max_run)) %>%
mutate(percent = c1 / total_max_run) %>%
mutate(pop1 = lag(percent), pop_cur = percent) %>%
# Identify if there is a CU that hits the 80% cutoff
mutate(cutoff = ifelse(pop_cur >= 0.8 & pop1 < 0.8, TRUE, FALSE)) %>%
ungroup() %>%
# Reorder the columns to make checking easier
select(CUID, Region, Species, Year, Total.run, c1, total_max_run, percent, pop1, pop_cur, cutoff)
View(cu_dat)
View(max_runs)
View(total_max_run)
rm(list = ls(all=TRUE)); #Remove all the objects in the memory
setwd("~/Dropbox (Salmon Watersheds)/X Drive/1_PROJECTS/Population Methods and Analysis/ProgRegSummaries")
library(tidyverse)
library(broom)
library(geosphere)
#read in CU level data
cc_file <- read.csv("data/dataset_1part1.Dec072020_CC.csv", header = T)
fraser_file <- read.csv("data/dataset_1part1.Jul282020_Fraser.csv", header = T)
vimi_file <- read.csv("data/dataset_1part1.Jul282021_VIMI.csv", header = T)
nass_file <- read.csv("data/dataset_1part1.Dec092020_Nass.csv", header = T)
skeena_file <- read.csv("data/dataset_1part1.Dec092020_Skeena.csv", header = T)
hg_file <- read.csv("data/dataset_1part1.Aug242020_HG.csv", header = T)
columbia_file <- read.csv("data/dataset_1part1.NOV272019_Columbia.csv", header = T)
# combine files from each region
cu_dat <- rbind(cc_file,fraser_file,vimi_file,nass_file,skeena_file,hg_file,columbia_file)
cu_dat <- select(cu_dat,CUID,Species,Year,Total.run,Region)
#check species names
unique(cu_dat$Species)
cu_dat$Species[cu_dat$Species=="River Sockeye"] <- "Sockeye"
cu_dat$Species[cu_dat$Species=="Lake Sockeye"] <- "Sockeye"
#### 2 Generate files needed for Provincial summaries
#Select only the years that have CUs that comprise 80% of the maximum historical run size
# ------------------------------------------------------------------------------
cu_dat <- as_tibble(cu_dat)
max_runs <-
cu_dat %>%
drop_na(Total.run) %>%
group_by(CUID, Species) %>%
slice(which.max(Total.run)) %>%
dplyr::rename(max_run = Total.run) %>%
ungroup()
total_max_run <-
max_runs %>%
group_by(Species, Region) %>%
summarise(total_max_run = sum(max_run)) %>%
ungroup()
test <-
cu_dat %>%
drop_na(Total.run) %>%
left_join(., max_runs %>% select(-Year)) %>%
left_join(., total_max_run) %>%
group_by(Year, Species, Region) %>%
arrange(Year, Species, Region, -max_run) %>%
mutate(c1 = cumsum(max_run)) %>%
mutate(percent = c1 / total_max_run) %>%
mutate(pop1 = lag(percent), pop_cur = percent) %>%
# Identify if there is a CU that hits the 80% cutoff
mutate(cutoff = ifelse(pop_cur >= 0.8 & pop1 < 0.8, TRUE, FALSE)) %>%
ungroup() %>%
# Reorder the columns to make checking easier
select(CUID, Region, Species, Year, Total.run, c1, total_max_run, percent, pop1, pop_cur, cutoff)
test$cutoff[is.na(test$cutoff)] <- TRUE
years_min80 <-
test %>%
slice(which(cutoff == TRUE)) %>%
select(Species, Year, Region)
output <- left_join(years_min80, test)
## create column for raw run size
d1<- na.omit(cu_dat) %>%
group_by(., Species, Region, Year) %>%
summarise(prov_runsize_raw = sum(Total.run))
